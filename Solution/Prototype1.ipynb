{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACS6420 Advanced project\n",
    "## Rolls-Royce Undergraduate project - Anomaly detection in discrete sequences\n",
    "### Author: Aldair M Silva\n",
    "### Supervisor: Visakan\n",
    "Date created: 13/10/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated:  11/03/2022\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "print('Updated: ', date.today().strftime('%d/%m/%Y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import urllib.parse\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "import io\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import csv\n",
    "from sklearn.manifold import TSNE\n",
    "#import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_raw = 'normalTrafficTraining.txt'\n",
    "anomaly_raw = 'anomalousTrafficTest.txt'\n",
    "\n",
    "normal_parse = 'normalRequestTraining.txt'\n",
    "anomaly_parse = 'anomalousRequestTest.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File handling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File parsing function\n",
    "def parse_file(file_in, file_out):\n",
    "    fin = open(file_in)\n",
    "    fout = io.open(file_out, \"w\", encoding=\"utf-8\")\n",
    "    lines = fin.readlines()\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i].strip()\n",
    "        if line.startswith(\"GET\"):\n",
    "            res.append(\"GET\" + line.split(\" \")[1])\n",
    "        elif line.startswith(\"POST\") or line.startswith(\"PUT\"):\n",
    "            url = line.split(' ')[0] + line.split(' ')[1]\n",
    "            j = 1\n",
    "            while True:\n",
    "                if lines[i + j].startswith(\"Content-Length\"):\n",
    "                    break\n",
    "                j += 1\n",
    "            j += 1\n",
    "            data = lines[i + j + 1].strip()\n",
    "            url += '?' + data\n",
    "            res.append(url)\n",
    "    for line in res:\n",
    "        line = urllib.parse.unquote(line).replace('\\n','').lower()\n",
    "        fout.writelines(line + '\\n')\n",
    "    print (\"finished parse \",len(res),\" requests\")\n",
    "    fout.close()\n",
    "    fin.close()\n",
    "\n",
    "# File reading function\n",
    "def loadData(file):\n",
    "    with open(file, 'r', encoding=\"utf8\") as f:\n",
    "        data = f.readlines()\n",
    "    result = []\n",
    "    for d in data:\n",
    "        d = d.strip()\n",
    "        if (len(d) > 0):\n",
    "            result.append(d)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished parse  36000  requests\n",
      "finished parse  25065  requests\n"
     ]
    }
   ],
   "source": [
    "normal_data = parse_file(normal_raw,normal_parse)\n",
    "anomaly_data = parse_file(anomaly_raw,anomaly_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_requests = loadData('normalRequestTraining.txt')\n",
    "bad_requests = loadData('anomalousRequestTest.txt')\n",
    "\n",
    "all_requests = bad_requests + good_requests\n",
    "yBad = [1] * len(bad_requests)\n",
    "yGood = [0] * len(good_requests)\n",
    "y = yBad + yGood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19825, 40810, 13985, 56155, 41366, 34993, 47123, 12015,  9371,\n",
       "       18377,  7074,  8411, 19187, 37922, 55546, 11500, 23583, 31497,\n",
       "       17194, 18707, 14790, 13101, 13535, 40994,  7326, 19475,   911,\n",
       "        8335,  5551, 30704, 28256, 38993, 11539, 19457, 31800, 14965,\n",
       "       34894, 57205, 35050, 60210, 34747, 26034, 42579, 50279, 12181,\n",
       "       17361, 14405, 26966,  8383, 34960, 40629, 43941,  9782, 33104,\n",
       "       14635, 46941, 44933, 32382, 54010,  1939, 57307, 48386, 60780,\n",
       "       29080, 33868, 21737, 25505,   816,  7644, 55792, 17543,  9753,\n",
       "       37990, 58398,  2692, 35998, 59897, 35616, 42896, 10259,  9739,\n",
       "        9272, 49830, 19619, 10228, 15164, 10821, 31573, 59126, 26761,\n",
       "        2502, 47211, 31509, 52126, 10566, 25332,  9619,   101, 18019,\n",
       "       36711, 11209,  9969, 25248,  4522, 41052, 54301,  2165, 47091,\n",
       "        7997, 42593, 41223, 39226,   847, 34618, 45772, 22223, 30643,\n",
       "       45098, 31059, 41662, 54132,  8835, 48943, 19057, 26856,  1862,\n",
       "       25136, 60979, 12980, 43769, 36981, 18143,  5858,  5761, 37799,\n",
       "       31675,  9036,  6578, 51053, 22309, 32868,  6887, 47764, 10399,\n",
       "       20558, 30212, 46188, 45951, 59877, 40763,  6972,  1972, 10407,\n",
       "        8598, 15924, 41528, 20260, 16516, 13570, 11782, 52530, 51953,\n",
       "       12517, 19085, 17274, 15925, 31637,  4661, 15151,    45, 60793,\n",
       "       57834, 46459, 46548, 13327, 19712,  1969, 39726, 15988, 32129,\n",
       "        3601, 41383, 54537,  8458, 59505, 24490,  1765, 60862, 39980,\n",
       "       51032, 21584, 26595, 30504, 11785, 33904,   240, 11062, 13597,\n",
       "       20017, 11172, 26008, 16945, 53796, 19616, 40119, 36574, 31268,\n",
       "       52310, 30552, 54570,  5434, 54051, 16783, 20327, 22197, 50571,\n",
       "       59967,  7323, 48467, 26574, 11464, 18561, 31106, 20601, 13210,\n",
       "         601, 34866, 30926, 30349, 11290, 28429, 22458, 34853, 53983,\n",
       "       26310, 27834, 12422, 13671, 59393, 30185, 29573, 20489,  8043,\n",
       "       48666, 16710, 40689, 28209, 24700,  2184, 35435, 46361, 35048,\n",
       "       59112,  3380, 55716, 51417, 46241, 36350, 43018, 45314, 29591,\n",
       "       18919,  5154, 40519, 20658,  2639,  6353, 21360, 23663,  4327,\n",
       "       42813, 19052, 11271,  3138, 38756,  7392, 26306,   820,  5264,\n",
       "       30736, 37214, 58336,  6693, 47726, 20028, 57609, 49830, 29936,\n",
       "       43682, 48714, 32237, 42199, 41816, 32894, 32976,  6851, 24673,\n",
       "        6081, 18926, 10655])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_random = np.random.randint(0,len(all_requests), 300)\n",
    "short_all_request = [all_requests[i] for i in idx_random]\n",
    "y_short = [y[i] for i in idx_random]\n",
    "idx_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_all_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vector\n",
    "\n",
    "    This section uses TF-IDF (Term frequency-Inverse Document Frequency) to transform the words in the document into numberical values.\n",
    "The values are determines by how many times they appear in the document, and the inverse frequency of the same word in other documents/corpus.\n",
    "This process is called \"vectorization\", and is calculated as tf=log[1+fre(term,document)], idf=log(1+total_number_of_pages/number_of_pages_containing_term).\n",
    "Both resutls are then multiplied. The values range from 0 to 1, 0 being very relevant and 1 being less relevant.\n",
    "Relevance in this case means the word is unique (less common/less frequent) in and across documents, and the opposite for less relevant words (such as what/the/is/in for instance).\n",
    "\n",
    "For this text analysis, the words will be taken as characters due to the nature of the document (web request), resulting in roughly 62 features extracted from 61065 requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61065, 62)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(all_requests)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "pd.DataFrame(X_train[:])\n",
    "X.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature analysis will show all the characters used as features. As it can be observed below, numbers, letters, special and unknwon characters are taken into account. \n",
    "The unknown features in this model exist because the vectorizer works in english language, and the document contains latin words/characters, although not very frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:  ['!' '\"' '#' '$' '%' '&' \"'\" '(' ')' '*' '+' ',' '-' '.' '/' '0' '1' '2'\n",
      " '3' '4' '5' '6' '7' '8' '9' ':' ';' '<' '=' '>' '?' '@' '_' 'a' 'b' 'c'\n",
      " 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u'\n",
      " 'v' 'w' 'x' 'y' 'z' '|' '~' 'ï¿½']\n"
     ]
    }
   ],
   "source": [
    "# Analysis of features extracted\n",
    "\n",
    "feature_array = np.array(vectorizer.get_feature_names_out())    # Get feature names\n",
    "print(\"Feature names: \", feature_array)\n",
    "\n",
    "X_dense = X.todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  5,  6,  9, 10, 13, 14, 15, 16, 17, 20, 23, 25, 26, 28, 30, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51,\n",
       "       52, 53, 55, 61], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(X_dense[0,:])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X vector matrix: \n",
      " [[0.         0.         0.         0.         0.18097812 0.21023937\n",
      "  0.30790935 0.         0.         0.23630751 0.31051935 0.\n",
      "  0.         0.04615579 0.11910886 0.07728439 0.07973636 0.09084181\n",
      "  0.         0.         0.10557915 0.         0.         0.09579201\n",
      "  0.         0.07728439 0.35438396 0.         0.18346481 0.\n",
      "  0.07029278 0.         0.         0.16610625 0.14655533 0.13446727\n",
      "  0.14289961 0.15074784 0.09395338 0.         0.09579201 0.16082227\n",
      "  0.0900221  0.1413723  0.13446727 0.14265478 0.12985367 0.16272356\n",
      "  0.127431   0.         0.19422    0.13446727 0.15074784 0.12476878\n",
      "  0.         0.10387202 0.         0.         0.         0.\n",
      "  0.         0.25187405]]\n",
      "\n",
      "Max value in vector:  0.7642262116462106\n",
      "Min value in vector:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"X vector matrix: \\n\", X_dense[1,:])\n",
    "print(\"\\nMax value in vector: \", np.max(X_dense))\n",
    "print(\"Min value in vector: \", np.min(X_dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features.csv gets created in the current working directory\n",
    "#with open('Features.csv', 'w', newline = '') as csvfile:\n",
    "#    my_writer = csv.writer(csvfile, delimiter = ' ')\n",
    "#    my_writer.writerow(X_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Explore how components affect model dimensions.\n",
    "Understand how relevant the method is to identify best features that can best pick anomalous data.\n",
    "Check explained variance ratios, try using a treshold to separate best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Audaz\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:585: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\Users\\Audaz\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Audaz\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Audaz\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:585: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "short_X_dense = X_dense[:300,:]\n",
    "pca = PCA(n_components=3, svd_solver='full')\n",
    "X_PCA = pca.fit_transform(short_X_dense)\n",
    "X_embedded = TSNE(n_components=2).fit_transform(short_X_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand and evaluate the meaning of variance in PCA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance ratio:  [0.2378072  0.13311821 0.0911572 ]\n",
      "Singular values:  [4.83715012 3.61906018 2.99483264]\n"
     ]
    }
   ],
   "source": [
    "print(\"Variance ratio: \", pca.explained_variance_ratio_)\n",
    "print(\"Singular values: \", pca.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "#plt.scatter(X_PCA[:,0], X_PCA[:,1], c=y_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear plot\n",
    "#plt.scatter(X_embedded[:,0], X_embedded[:,1], c=y_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Logistic Regression : 0.7995087336244542\n",
      "Confusion Matrix: \n",
      "[[9742 1103]\n",
      " [2570 4905]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Audaz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lgs = LogisticRegression()\n",
    "lgs.fit(X_train, y_train)\n",
    "y_pred = lgs.predict(X_test)\n",
    "score_test = metrics.accuracy_score(y_test, y_pred)\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print (\"Score Logistic Regression :\",score_test)\n",
    "print (\"Confusion Matrix: \")\n",
    "print (matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree model\n",
    "Write on understanding of rules.\n",
    "Plot decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Decesion Tree : 0.9671397379912664\n",
      "Confusion Matrix: \n",
      "[[10537   308]\n",
      " [  294  7181]]\n"
     ]
    }
   ],
   "source": [
    "dtc = tree.DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "y_pred = dtc.predict(X_test)\n",
    "score_test = metrics.accuracy_score(y_test, y_pred)\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print (\"Score Decesion Tree :\",score_test)\n",
    "print (\"Confusion Matrix: \")\n",
    "print (matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision trees\n",
    "tree.plot_tree(dtc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Linear SVM : 0.8186681222707424\n",
      "Confusion Matrix: \n",
      "[[9830 1015]\n",
      " [2307 5168]]\n"
     ]
    }
   ],
   "source": [
    "linear_svm=LinearSVC(C=1)\n",
    "linear_svm.fit(X_train, y_train)\n",
    "y_pred = linear_svm.predict(X_test)\n",
    "score_test = metrics.accuracy_score(y_test, y_pred)\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print (\"Score Linear SVM :\",score_test)\n",
    "print (\"Confusion Matrix: \")\n",
    "print (matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest model\n",
    "\n",
    "Write on rules used for random forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Random Forest : 0.9811681222707423\n",
      "Confusion Matrix: \n",
      "[[9830 1015]\n",
      " [2307 5168]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "score_test = metrics.accuracy_score(y_test, y_pred)\n",
    "print (\"Score Random Forest :\",score_test)\n",
    "print (\"Confusion Matrix: \")\n",
    "print (matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random forest"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
